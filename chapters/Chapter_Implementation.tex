\chapter{Implementierung}\label{ch:implementation}



%Für unsere Implementation wird für das Zwischenspeichern von Daten ein Serviceworker eingesetzt. Serviceworker können wie ein Proxy zwischen dem Webbrowser und dem Webserver agieren, welcher die Webseite bereitstellt. Stellt ein Browser eine Anfrage, so wird diese vom Serviceworker abgefangen. Der Serviceworker schaut zunächst in seinem Cache, der sog. IndexDB, ob er die gestellte Anfrage beantworten kann. Ist dies nicht der Fall, so wird die Anfrage an den Webserver weitergeleitet. Wird die gleiche Anfrage nochmals gestellt, kann diese aus dem Cache beantwortet werden, da gestellte Anfragen eine gewisse Zeit lang zwischengespeichert werden.
%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.8\textwidth]{figures/ServiceWorker}
%	\caption[A Figure Short-Title]{A Figure Title}
%	\label{fig:sequenceDiagram}
%\end{figure}
%
%
%Die von uns eingesetzte Technologie zur Übertragung von Daten zwischen Browsern ist WebRTC. WebRTC ist ein offener Standard und ermöglicht es Browser paarweise zwecks Datenaustausch zu verbinden. Der große Vorteil dieser Technologie ist, dass sie direkt von modernen Browsern unterstützt wird, wodurch keine zusätzliche Software installiert werden muss. Konkret wird von uns ein sog. DataChannel genutzt.
%
%Für den Datenaustausch müssen wechselseitig DataChannel zueinander aufgebaut werden. Die Ausgangslage ist, dass die Schüler wissen, dass es den anderen gibt, aber nicht wie der jeweils andere zu erreichen ist. Um diese Problematik zu lösen, existiert ein Vermittlungsserver (Signaling server).
%
%Als erstes werden Informationen, über die Verbindung die aufgebaut werden soll, an den Signaling server gesendet. Technisch wird ein SDP-offer gesendet, wobei SDP für Session Description Protocol steht. Dieses SDP-offer leitet der Signaling server an die Schüler in der Klasse/Schule weiter. Geantwortet wird mit einer SDP-answere, welche Informationen über die abgestimmte Verbindung enthält und über den Signaling server zurück geleitet wird.
%
%Damit eine direkte Verbindung aufgebaut werden kann, müssen über den Signaling server noch weitere Informationen wie ICE-Kandidaten ausgetauscht werden. ICE steht hierbei für Interactive Conectivity Establishment und ist fester Bestandteil von WebRTC. Es ist für den Aufbau der Browser-zu-Browser-Verbindung verantwortlich. ICE-Kandidaten enthalten hauptsächlich Informationen darüber wie ein bestimmter Nutzer erreichbar ist (also z.B. private oder öffentliche IP-Adresse). Ermittelt werden diese ICE-Kandidaten mithilfe eines STUN-Servers und dem dazugehörigen Session Traversal Utilities for NAT (STUN) Protokoll. Wie der Name des Protokolls schon verrät, wird es vor allem benötigt um auch Nutzer erreichen zu können die keine eigene öffentliche IP-Adresse besitzen, bei denen also Network address translation (NAT) eingesetzt wird. Dies ist aufgrund der mangelnden Anzahl an IPv4-Adressen bei fast jedem Internetnutzer der Fall.
%
%In dem Signaling server selbst wird die Logik abgebildet, wie die Klassen und Schüler miteinander in Verbindung stehen. Implementiert wurde dieser mit socket.io, da die native Klassenorganisation und Websocket-Technologie sich nahezu perfekt für unser Szenario anbot.

\section{Architektur}

\begin{itemize}
  \item Technologiewahl
  \item ES6 compilation
  \item Beschreibung der Komponenten:
  \item Service Worker
  \item 	Cached Ressourcen
  \item 	Proxy
  \item P2PCDN
  \item 	Schnittstelle für externe Anwendungen
  \item 	Nimmt Konfiguration entgegen
  \item 	Initialisiert CDN
  \item Middleware
  \item 	Vermittler zwischen Service Worker und Script
  \item 	Arbeitet mit Events
  \item 	Welche Events gibt es wer registriert sich drauf?
  \item Peer
  \item 	Representiert eigenen Peer
  \item 	Hält verbindungen zu anderen Peers
  \item Signaling
  \item FayeConnection	
\end{itemize}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/Klassendiagramm}
	\caption[A Figure Short-Title]{Klassendiagramm}
	\label{fig:Klassendiagramm}
\end{figure}


\subsection{Service worker}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/ServiceWorker}
	\caption[A Figure Short-Title]{A Figure Title}
	\label{fig:sequenceDiagram}
\end{figure}
\begin{itemize}
  \item warten bis sw Aktiv ist/alle verbindungen ready sind vs request normal durchgehen lassen bis alles fertig ist.
  \item 	Austausch erst möglich wenn script geladen ist
  \item 	blockent: 
  \item 		Ressourcen werden so lange aufgehalten bis verbindung initialisiert
  \item 		--> längere Ladezeiten
  \item 		mehr Bandbreite kann gespart werden
  \item 	nicht blockent: 
  \item 		Ressourcen werden über server geladen bis verbinungen aufgebaut sind
  \item 		Weniger Bandbreite wird gespart
  \item 		Ladezeit wird nicht verzögert
  \item 		Heartbeat um zu überprüfen ob script geladen ist
  \item Mehrere Tabs?
  \item 		
  \item first page load
  \item Single page apps
  \item Turbolinks
  \item Requests werden erst nach erfolgreicher registrierung behandelt
  \item clients.claim + skip waiting für den ersten aufruf (12)
  \item Codebeispiel für wartende messages
  \item 	Callbacks
\end{itemize}

\subsection{Tests}
Um das Plugin zu testen wird als test Runner Karma verwendet.\footnote{https://karma-runner.github.io/latest/index.html} Als test framework wird mochajs\footnote{https://mochajs.org/} und als assertion Library wird Chai\footnote{https://www.chaijs.com/} eingesetzt 

%- mocha
%- chai
%- karma
% Event handler testen
% 	- registrieren/abmelden --> test helper

\section{Ressourcen Management}



\section{Configuration}
\begin{listing}[h]
	\inputminted{javascript}{listings/configuration.js}
	\caption{Beispielhafte Konfiguration}
	\label{lst:configuration}
\end{listing}

Um eine gute Anpassung an verschiedene Anwendungsfälle zu ermöglichen bietet das \pTp CDN eine Reihe von Konfigurationsmöglichkeiten. Nachdem das Client Script die Konfiguration geladen hat wird sie im der Indexed DB gespeichert und von dort durch den Service Worker geladen.\ref{lst:configuration} zeigt eine Beispielhaft Konfiguration des CDNs im Folgenden werden die verschiedenen Konfigurationsmöglichkeiten aufgelistet und beschrieben.

\begin{description}
\item[channel]\hfill \\
Bezeichnet den für das Peer Meshing zu verwendenden Websocket channel. Alle Clients mit dem selben Channel befinden sich im selben Peer Mesh.
\item[clientId]\hfill \\
Eindeutiger Identifizier mit dem der Peer identifiziert werden kann. Ähnlich einer Session ID wird er verwendet um Clients wieder zuerkennen und anzusprechen.
\item[idLength]\hfill \\
Bezeichnet die maximale Länge der ClientIds. Kürzere ClientIds werden bis zu dieser Länge aufgefüllt. Wird benötigt um intern bei dem verschicken von Paketen über das CDN ClientIds fester länge verwenden zu können.(siehe \todo{referenzieren}) 
\item[stunServer]\hfill \\
Gibt den zu verwendenden STUN Server an. Dies kann ein öffentlicher oder privat betriebener Server sein. Kann freigelassen werden falls kein STUN Server verwendet werden soll
\item[verbose]\hfill \\
Aktiviert/deaktiviert Debug Ausgaben.
\item[serviceWorker]\hfill \\
Beinhaltet alle Konfigurationen die den Service Worker betreffen.
\item[urlsToShare]\hfill \\
Liste aller URL die mit Hilfe des CDN bearbeitet werden sollen.
\item[excludedUrls]\hfill \\
Liste von URLs die explizit von dem CDN ausgeschlossen werden sollen
\item[path]\hfill \\ und Text dahinter
Pfad von dem das Service Worker Script geladen werden soll.
\item[scope]\hfill \\
Gibt den Scope an unter dem der Service Worker arbeiten soll.
\item[basePath]\hfill \\
Gibt den Service Worker Base Path an.
\item[storageQuota]\hfill \\
Maximal für das CDN zu verwendender Cache Speicher. Überschreitet der Cache den Wert, werden so lange Ressourcen aus dem Cache gelöscht bis der Wert unterschritten ist. (siehe \todo{ref}) 
\item[cachingEnabled]\hfill \\
Aktiviert/Deaktiviert das Caching innerhalb des Service Workers. Nützlich zum Debuggen.
\item[verbose]\hfill \\
Gibt an ob der Service Worker debugging Ausgaben auf die Konsole schreiben soll.
\item[statisticPath]\hfill \\
URL an die die erhobenen Statistiken gesendet werden sollen. siehe \ref{ch:implementation:stats}

\end{description}

\section{Client UI Event}
Um es der einbindenden Anwendung zu ermöglichen auf Änderungen bezüglich der verbundenen Peers sowie deren Ressourcen zu Reagieren stellt das Plugin das Event ui:onUpdate bereit das bei Änderungen ausgelöst wird. Das Event übergibt das Peer Object des Clients wodurch der Event Empfänger zugriff auf die Anzahl der verbundenen Peers so wie deren Ressourcen hat. 

\section{Signaling Server}
The signaling server itself uses socket.io and can be found here. The client ID is created there and is essential for the lifecycle of a peer in the whole network. It is not clear if the client IDs given by socket.io always have the same length. Therefore, client IDs will be padded to a maximal length of 24. This is necessary because the client IDs need to be sent via the binary datachannel and consequently, this requires a fixed length.


\begin{itemize}
  \item websockets faye
  \item gleicher data channel zur Bildung von meshs
  \item ID pro client
  \item max id length muss festgelegt werden für chunking
  \item Protokoll von P2PCDN umgesetzt
  \item Protokoll erklären
  \item Diagram Protokoll
  \item Peer Meshing wird von anwendung übernommen
\end{itemize}

\subsection{Slidesync}
\begin{itemize}
	\item Rails Model
	\item Code Beispiel
	\item Tracking in Redis welcher Peer in welchem Submesh ist
	\item Peer wird aus liste nicht voller mehses gewählt
	\item Falls alle voll sind wird neues Mesh angelegt und garbage collection gestartet.
	\item Datentyen und mapping erklären
	\item Background job räumt leere meshes auf (Garbage collection)
	\item 	löscht leere meshes aus liste
\end{itemize}

\subsection{\schulCloud}

\section{Message protocol}
%In unserer Implementation wird, sobald eine neuer Besucher der Webseite hinzukommt, sofort ein DataChannel, mittels WebRTC, STUN, ICE und Signaling server zu allen anderen aktiven Besuchern aufgebaut. Über diesen werden zu zwei Zeitpunkten Informationen darüber ausgetauscht, welche Ressourcen bei dem jeweiligen Nutzer vorliegen: Direkt nach Aufbau des DataChannels und immer dann, wenn ein Nutzer eine neue Ressource (aus dem Internet oder lokal) geladen und in seinem Cache gespeichert hat:

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/SequenceDiagram}
	\caption[A Figure Short-Title]{A Figure Title}
	\label{fig:sequenceDiagram}
\end{figure}
\todo{Diagramm aktualisieren}

Client 1 (C1) ist der erste der die Webseite aufruft. Er registriert sich beim Signaling server und fragt im Anschluss img.png an (rot). Da noch niemand anders auf der Seite ist von dem er die Ressource bekommen könnte und er zudem die Ressource nicht in seinem Cache hat, wird img.png über das Internet vom Webserver geladen. Client 2 (C2) ruft nun ebenfalls die Webseite auf und registriert sich beim Signaling server. Dieser benachrichtigt C1, dass ein neuer Teilnehmer registriert wurde, woraufhin C1 einen Verbindungsaufbau zu C2 einleitet. Steht die direkte Verbindung zwischen C1 und C2 (grün), teilt C1 C2 den Inhalt seines aktuellen Caches mit. Fragt C2 img.png an (rot), weiß er so, dass er diese von C1 anfragen kann. Hat er img.png erhalten, teilt er allen anderen Teilnehmern (in diesem Fall nur C1) mit, dass auch er jetzt img.png als Ressource in seinem Cache hat

\subsection{Updates}
\begin{itemize}
	\item message types
	\item neue Resource
	\item gelöschte Resource
	\item Client tritt dem Netzwerk bei
	\item Client verlässt Netzwerk
	\item Diagram aktualisieren
	\item alle Clients müssen benachrichtigt werden
	\item Über Webrtc data channel
	\item Message format zeigen
\end{itemize}
\subsection{Client fragt Ressource an}
\begin{itemize}
	\item Message Format
	\item Diagram beschreiben
\end{itemize}
\section{Mesh Zuordnung}
% data Channels zur mesh verbindung definiert über Anwendung
% ip subnetz erkennung wie implementiert?

\section{Reusability}

\section{Serialisierung der Daten}
Für den Austausch von Daten werden die von WebRTC angebotene DataChannel genutzt, welche das Stream Control Transmission Protocol kurz SCTP verwendet. Problem hierbei ist, dass dieses Protokoll ursprünglich für die Übertragung von Kontrollinformationen designt wurde und deshalb für die Kompatibilität verschiedener Browser eine Paketgröße von 16kiB nicht überschritten werden sollte. Es ist jedoch notwendig auch größere Dateien zu übertragen, weshalb aktuell viele kleine Datenpakete verwendet werden müssen. Hierdurch entsteht ein nicht zu vernachlässigender Overhead.

\begin{listing}[h]
	\inputminted{javascript}{listings/buffersize.js}
	\caption{Buffersize Berücksichtigung}
	\label{lst:code-buffersize}
\end{listing}
\begin{listing}[h]
	\inputminted{javascript}{listings/handle_chunk.js}
	\caption{Buffersize Berücksichtigung}
	\label{lst:code-handle-chunk}
\end{listing}

\begin{itemize}
  \item SCTP(DataChannel) Message size limit 16kiB
  \item Fixe id länge notwendig
  \item datachannel buffersize muss berücksichtigt werden
  \item chunking reassembling nötig
  \item Performance einbussen
  \item abToMessage und sendToPeer
\end{itemize}

% SCTP(DataChannel) message size limit 16kiB
% chunking reassembling nötig
% _abToMessage() und _sendToPeer
% Performance einbußen

%Currently, it is only possible to send messages not larger than 16kiB via the RTCDataChannel. In order to send larger messages chunking and reassembling is necessary. These procedures take place in the _abToMessage() and _sendToPeer() methods in the peer.js file.

\begin{itemize}
	\item Details zum Algorithmus
\end{itemize}
\section{System Test}
Das Plugin stellt ein Modul bereit um zu testen ob es einem Client möglich ist am \pTp CDN teilzunehmen.

Dazu werden drei Tests bereitgestellt.
\begin{lstlisting}
	p2pCDN.systemTest.testBrowser()
\end{lstlisting}
 Mit Hilfe von modernizr\footnote{https://modernizr.com/} testet die Funktion ob die verwendete Browserversion alle benötigten Funktionen unterstützt. Modernizr ist eine Javascript Bibliothek mit der getestet werden kann ob ein Browser bestimmte Funktionen unterstützt.
\begin{lstlisting}
	p2pCDN.systemTest.webrtcInitialized()
\end{lstlisting}
Gibt ein Promise zurück welches überprüft ob die webrtc Verbindung erfolgreich aufgebaut wurde. Da die Initialisierung einen Moment in Anspruch nehmen kann wird wiederholt geprüft ob die Verbindung aufgebaut wurde.
\begin{lstlisting}
	p2pCDN.systemTest.clientConnected()
\end{lstlisting}
Gibt ebenfalls ein Promise zurück und überprüft ob erfolgreich eine Verbindung zu einem anderen Client aufgebaut werden konnte. Um diesen Test erfolgreich auszuführen muss sich ein andere Client im aktuellen Peer mesh befinden.

\section{Erfassen von Statistiken}\label{ch:implementation:stats}
\begin{listing}[h]
	\inputminted{javascript}{listings/sendStatistic.js}
	\caption{Erfassen der Statistiken}
	\label{lst:code-stats}
\end{listing}
\begin{listing}[h]
	\inputminted{javascript}{listings/logStatistic.js}
	\caption{Erfassen der Statistiken}
	\label{lst:code-stats}
\end{listing}
\begin{listing}[h]
	\inputminted{javascript}{listings/handle_request.js}
	\caption{Abarbeitung eines Request im Service Worker}
	\label{lst:handle_request}
\end{listing}

Um die Nutzungsstatistiken des CDNs zu erfassen sendet jeder client periodisch POST requests an den Server. Dazu sammelt der Service Worker alle Anfragen die in einem Zeitraum von 10 Sekunden angefallen sind und sendet sie gebündelt als JSON an den Server.
Erfasst werden:

\begin{description}
\item[peerId]\hfill \\
Die PeerId bezeichnet die Id des peers der die Statistik sendet.
\item[method]\hfill \\
Method gibt an wie die Anfrage behandelt wurde und kann die Werte 'cacheResponse', 'peerResponse' oder 'serverResponse' beinhalten. Ein cacheResponse konnte aus dem eigenen Cache beantwortet werden. ServerResponse bedeutet das die Anfrage über den externen Server geladen werden musste. Der Wert peerResponse gibt an das die Anfrage über das \pTp CDN bearbeitet werden konnte.
\item[from]\hfill \\
 'From' gibt an woher die Anfrage geladen wurde. Im Falle eines serverResponses beinhaltet sie den Wert 'server' und bei einem cacheResponse den Wert cache. Wurde die Anfrage über das \pTp Netzwerk beantwortet beinhaltet sie die peerId des Peers der die Anfrage beantwortet hat. Dazu sendet das Script neben der eigentlichen Anfrage auch die eigene PeerId und die PeerId des peers der die Anfrage beantwortet hat an die Service Worker. (siehe \ref{lst:handle_chunk}) 
\item[URL]\hfill \\
Die URL enthält die URL der angefragten Ressource. 
\item[timing]\hfill \\
Timing beinhaltet die Zeitspanne die benötigt wurde um die Anfrage zu beantworten, beginnend vor der Entscheidung wie der Request abgearbeitet werden soll(siehe \ref{lst:handle_request}) und endend nach dem die Anfrage empfangen wurde. Nicht enthalten in der Zeitspanne ist die Entscheidung ob der Service worker den Request bearbeitet und die Renderzeiten des Browsers. Diese Zeiten sind nicht abhängig von der Art der Request Beantwortung.(siehe Evaluation)
\end{description}
Mit Hilfe der Configuration kann festgelegt werden an welchen Endpunkt die Statistik gesendet werden soll. Die Anwendung ist für das speichern und verarbeiten der Daten zuständig, dies ist nicht Teil des Plugins. Slidesync speichert die Daten als JSON in Redis und stellt einen JSON Endpunkt zur Verfügung mit dem die Statistiken abgerufen werden können. Für die Labortests werden die Daten in JSON Dateien zur späteren Verarbeitung abgelegt.
\begin{listing}[h]
	\inputminted{javascript}{listings/_handle_chunk.js}
	\caption{}
	\label{lst:handle_chunk}
\end{listing}

\begin{itemize}
	\item 	evtl. Mongo db
	\item 	anzeigetool
	\item schoolcloud
\end{itemize}

\section{Quota limits - Löschen von Requests aus dem Cache}
\footnote{https://developers.google.com/web/fundamentals/instant-and-offline/web-storage/offline-for-pwa}
\begin{table}[!htb]
	\caption{Browser Storage Quotas }
\begin{center}

	\begin{tabular}{|r|l|l|}
		\hline
		Browser	 & Limit \\ \hline
		Chrome & < 6\% des freien Fesplattenspeichers \\ \hline
		Firefox & < 10\% des freien Fesplattenspeichers \\ \hline
		Safari & < 50MB \\ \hline
		IE10 & < 250MB \\ \hline
		Edge & Abhännig von der Festplattengröße \\ 
		\hline
	\end{tabular}
\end{center}
\end{table}

Browser stellen den Clients unterschiedlich viel Speicherplatz für offline Caches zur Verfügung. Ist das Quota limit erreicht versuchen Firefox und Chrome Speicher frei zu machen indem Elemente aus dem Cache gelöscht werden. Dabei werden jedoch keine einzelnen Elemente aus dem Cache gelöscht sondern mittels Last-recently-used(LRU) werden ganze Caches gelöscht. Safari und Edge haben keinen Mechanismus zum automatischen Löschen von Elementen sondern werfen lediglich einen Fehler.\footnote{https://developers.google.com/web/fundamentals/instant-and-offline/web-storage/offline-for-pwa} Deshalb ist es notwendig das der Service Worker in dem Fall das das Limit erreicht wird Elemente löscht.

Mit Hilfe der Quota Management API\footnote{https://developers.google.com/web/updates/2011/11/Quota-Management-API-Fast-Facts} ist es möglich die momentane Speichernutzung auszulesen ebenso wie den maximal Verfügbaren Speicherplatz. Ist diese Limit oder das Quota Limit welches über die Konfiguration angegeben wurde erreicht, löscht der Service Worker so lange die ältesten Einträge im Cache, bis genügend Speicher für den nächsten Request vorhanden ist. Dazu berechnet der Service Worker die Größe des zu speichernden Request. 

%	\item https://developers.google.com/web/updates/2017/08/estimating-available-storage-space
%	\item https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API/Browser_storage_limits_and_eviction_criteria

\section{Resource loading}

\section{ Configuration von Unternehmensnetzwerken }

\begin{itemize}
	\item Port range
	\item firewalls?w
\end{itemize}